{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gental Introduction To torch.autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.autograd` is PyTorch's automatic differentiation engine which powers neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "Training a neural network happens in two steps:  \n",
    "* Forward propogation: run the input data and obtain output through the nested functions of this NN.\n",
    "* Backward propogation: NN adjusts its parameters proportionate to the error in its guess through gradient descent methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage in PyTorch\n",
    "For instance, we load a pretrained *resnet18* model from `torchvision`.  \n",
    "We create a random data tensor to represent a single image with 3 channels, and hight & weight of 64.  \n",
    "The corresponding label is initialized to some random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass: run the input data through the model through each of its layers to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the error (loss) between the model's prediction and the corresponding label.  \n",
    "\n",
    "Backward pass: we call `.backward()` on the error tensor. Autograd then calculates and stores the gradients for each model parameter in the parameter's `.grad` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load an optimizer, in this case SGD with a learning rate of 0.01 and momentum of 0.9. We register all the parameters of the model in the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we call `.step()` to initiate gradient descent. The optimizer adjusts each parameter by its gradients stored in `.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiation in Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How `autograd` collects gradients? Let's create two tensors with `requires_grad=True`, which tells `autograd` that every operation on these two tensors should be tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 3.], requires_grad=True) tensor([6., 4.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another tensor from `a` and `b`:\n",
    "$$Q = 3a^3-b^2.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-12.,  65.], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = 3 * a ** 3 - b ** 2\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-42395d6027c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mQ\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Softwares\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwares\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m     \u001b[0mgrad_tensors_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Softwares\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "Q.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why there is an error? Because we can only call `.backward()` only when the output (`Q` here) is a scalar. Now, since both `a` and `b` are two-dim tensors, thus `Q` is a vector,  then we need to pass a `gradient` argument in `Q.backward()`.  \n",
    "\n",
    "'gradient' is a tensor of the same shape as `Q`, here we choose $[1, 1]$. Before we give the reason, let's introduce the **vector calculus using `autograd`** first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Calculus using `autograd`\n",
    "Mathematically, if we have a vector valued function $\\vec{y}=f(\\vec{x})$, then the gradient matrix is called Jacobian matrix\n",
    "$$ \n",
    "J= \\left(\\begin{matrix}\n",
    "   \\frac{\\partial y_1}{\\partial x_1} & ... & \\frac{\\partial y_1}{\\partial x_n} \\\\\n",
    "   ... & ... & ... \\\\\n",
    "   \\frac{\\partial y_m}{\\partial x_1} & ... & \\frac{\\partial y_m}{\\partial x_n}\n",
    "  \\end{matrix}\n",
    "  \\right).\n",
    "$$\n",
    "Generally speaking, `torch.autograd` is an engine for computing vector-Jacobian product. That is, given any vector $\\vec{v}$, it returns $J^{T}\\vec{v}$.\n",
    "\n",
    "If $l$ is a scalar function w.r.t. $\\vec{y}$, namely $l=g(\\vec{y})$ and $\\vec{v}$ is the gradient of this scalar function, that is, \n",
    "$$\n",
    "\\vec{v} = \\left(\\begin{matrix}\n",
    "   \\frac{\\partial l}{\\partial y_1}  \\\\\n",
    "   ... \\\\\n",
    "   \\frac{\\partial l}{\\partial y_m} \n",
    "  \\end{matrix}\n",
    "  \\right),\n",
    "$$\n",
    "then \n",
    "$$ \n",
    "J^T\\vec{v}= \\left(\\begin{matrix}\n",
    "   \\frac{\\partial y_1}{\\partial x_1} & ... & \\frac{\\partial y_m}{\\partial x_1} \\\\\n",
    "   ... & ... & ... \\\\\n",
    "   \\frac{\\partial y_1}{\\partial x_n} & ... & \\frac{\\partial y_m}{\\partial x_n}\n",
    "  \\end{matrix}\n",
    "  \\right)\n",
    "  \\left(\\begin{matrix}\n",
    "   \\frac{\\partial l}{\\partial y_1}  \\\\\n",
    "   ... \\\\\n",
    "   \\frac{\\partial l}{\\partial y_m} \n",
    "  \\end{matrix}\n",
    "  \\right)\n",
    "  =\n",
    "  \\left(\\begin{matrix}\n",
    "   \\frac{\\partial l}{\\partial x_1}  \\\\\n",
    "   ... \\\\\n",
    "   \\frac{\\partial l}{\\partial x_n} \n",
    "  \\end{matrix}\n",
    "  \\right).  \n",
    "$$\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to the previous example $Q = 3a^3 - b^2$.  \n",
    "If we let $l=Q_1+Q_2$, then \n",
    "$\\vec{v}=\\left(\\begin{matrix}\n",
    "   1  \\\\\n",
    "   1\n",
    "  \\end{matrix}\n",
    "  \\right)$.\n",
    "We pass this $\\vec{v}$ as a `gradient` argument in `Q.backward()`, then its execution, we obtain the vector-Jacobian product \n",
    "$$\n",
    "\\left(\\begin{matrix}\n",
    "   \\frac{\\partial (Q_1+Q_2)}{\\partial a_1}  \\\\\n",
    "   \\frac{\\partial (Q_1+Q_2)}{\\partial a_2}  \\\\\n",
    "   \\frac{\\partial (Q_1+Q_2)}{\\partial b_1}  \\\\\n",
    "   \\frac{\\partial (Q_1+Q_2)}{\\partial b_2}  \n",
    "  \\end{matrix}\n",
    "  \\right)\n",
    "  =\n",
    "  \\left(\\begin{matrix}\n",
    "   \\frac{\\partial Q_1}{\\partial a_1}  \\\\\n",
    "   \\frac{\\partial Q_2}{\\partial a_2}  \\\\\n",
    "   \\frac{\\partial Q_1}{\\partial b_1}  \\\\\n",
    "   \\frac{\\partial Q_2}{\\partial b_2}  \n",
    "  \\end{matrix}\n",
    "  \\right),\n",
    "$$\n",
    "which comes from the fact that $Q_1 = 3a_1^3 - b_1^2$ and $Q_2 = 3a_2^3 - b_2^2$. Up to now, we obtain the gradients, which are stored in the corresponding tensor's `.grad` attribute, namely `a.grad` and `b.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if collected gradients are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "print(9 * a ** 2 == a.grad)\n",
    "print(-2 * b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently, we can find from the choice of $\\vec{v}$ that, we can directly call `Q.sum().backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "Q = 3 * a ** 3 - b ** 2\n",
    "Q.sum().backward()\n",
    "print(9 * a ** 2 == a.grad)\n",
    "print(-2 * b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graph\n",
    "Below is the visual representation of the DAG in the above example. DAG consists of *Function* objects. Leaves are the input tensors, roots are the output tensors.\n",
    "\n",
    "The arrows are in the direction of the forward pass. The nodes represent the backward functions of each operation in the forward pass. The leaf nodes in blue represent our leaf tensors `a` and `b`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DAG](dag_autograd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see `a` takes power first, then times 3, while `b` takes power and times 1. At last, there is a subtraction between $3a^3$ and $b^2$.  \n",
    "\n",
    "**DAGs are dynamic in PyTorch.** After each `.backward()` call, autograd starts populating a new graph. We can change the shape, size and operations at every iteration if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclusion from the DAG \n",
    "`torch.autograd` tracks operations on all tensors which have their `requires_grad` flag set to `True`. Setting this attribute to `False` excludes the tensor from the gradient computaion DAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We show that even if only a single input tensor has `requires_grad=True`, the output tensor still will require gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does 'a' require gradients? False\n",
      "Does 'b' require gradients? True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand(5, 5, requires_grad=True)\n",
    "\n",
    "a = x + y\n",
    "b = x + z\n",
    "print(f\"Does 'a' require gradients? {a.requires_grad}\")\n",
    "print(f\"Does 'b' require gradients? {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frozen parameters**: parameters that don't compute gradients. Useful if you know in advance that you won't need the gradients of those parameters, and reduce autograd computaions.  \n",
    "\n",
    "**Finetuning a pretrained network**: freeze most of the model and typically only modify the classifier layers to make predictions on new labels. As before, we load a pretrained resnet18 model, and freeze all the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to finetune the model on a new dataset with 10 labels. In resnet, the classifier is the last linear layer `model.fc`. We can simply replace it with a new linear layer (unfrozen by default) that acts as our classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the only parameters that compute gradients are the weights and bias of `model.fc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize only the classifier\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we register all the parameters in the optimizer, the only parameters that are computing gradients and hence updated in gradient descent are the weights and bias of the classifier.\n",
    "\n",
    "The same exclusionary functionality is available as a context manager in `torch.no_grad()`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
